{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Datasets Experiments - CNN vs Vision Transformer\n",
    "\n",
    "This notebook supports:\n",
    "- **Existing imbalanced datasets**: CIFAR-10-LT, CIFAR-100-LT (from Hugging Face)\n",
    "- **Custom imbalanced datasets**: Create your own imbalanced CIFAR-10/100\n",
    "\n",
    "Compare CNN (ResNet) and Vision Transformer (ViT) performance on various imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchmetrics pyyaml matplotlib seaborn datasets huggingface_hub -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload Project Files\n",
    "\n",
    "Upload the following files:\n",
    "- `CNN.py`, `VisionTransormer.py`, `config.yaml` (to root)\n",
    "- Files from `imbalanced-exp/` folder\n",
    "- `cka/metrics.py` (to `cka/` folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create necessary directories\n",
    "!mkdir -p imbalanced-exp cka data\n",
    "\n",
    "# Check for required files\n",
    "required_files = [\n",
    "    'CNN.py',\n",
    "    'VisionTransormer.py',\n",
    "    'config.yaml',\n",
    "    'imbalanced-exp/create_imbalanced_dataset.py',\n",
    "    'imbalanced-exp/evaluate.py',\n",
    "    'cka/metrics.py'\n",
    "]\n",
    "\n",
    "print(\"Checking required files...\")\n",
    "missing_files = []\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"✓ {file}\")\n",
    "    else:\n",
    "        print(f\"✗ {file} - MISSING!\")\n",
    "        missing_files.append(file)\n",
    "\n",
    "if missing_files:\n",
    "    print(\"\\n⚠ Please upload the missing files using the file browser on the left.\")\n",
    "else:\n",
    "    print(\"\\n✓ All required files are present!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Existing Imbalanced Datasets\n",
    "\n",
    "This section loads pre-existing long-tailed imbalanced datasets from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class HuggingFaceCIFAR(Dataset):\n",
    "    \"\"\"Wrapper for Hugging Face CIFAR datasets\"\"\"\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['img'] if 'img' in item else item['image']\n",
    "        if isinstance(image, dict):\n",
    "            image = Image.fromarray(np.array(image))\n",
    "        elif not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = item['label']\n",
    "        return image, label\n",
    "\n",
    "def load_cifar10_lt(imbalance_factor=100, split='train', transform=None):\n",
    "    \"\"\"\n",
    "    Load CIFAR-10-LT (Long-Tailed) dataset from Hugging Face.\n",
    "    \n",
    "    Args:\n",
    "        imbalance_factor: Imbalance factor (10, 50, 100, 200)\n",
    "        split: 'train' or 'test'\n",
    "        transform: Torchvision transforms\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset_name = f\"tomas-gajarsky/cifar10-lt-{imbalance_factor}\"\n",
    "        print(f\"Loading {dataset_name} ({split})...\")\n",
    "        hf_dataset = load_dataset(dataset_name, split=split)\n",
    "        return HuggingFaceCIFAR(hf_dataset, transform=transform)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CIFAR-10-LT: {e}\")\n",
    "        print(\"Falling back to creating custom imbalanced dataset...\")\n",
    "        return None\n",
    "\n",
    "def load_cifar100_lt(imbalance_factor=100, split='train', transform=None):\n",
    "    \"\"\"\n",
    "    Load CIFAR-100-LT (Long-Tailed) dataset from Hugging Face.\n",
    "    \n",
    "    Args:\n",
    "        imbalance_factor: Imbalance factor (10, 50, 100, 200)\n",
    "        split: 'train' or 'test'\n",
    "        transform: Torchvision transforms\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset_name = f\"tomas-gajarsky/cifar100-lt-{imbalance_factor}\"\n",
    "        print(f\"Loading {dataset_name} ({split})...\")\n",
    "        hf_dataset = load_dataset(dataset_name, split=split)\n",
    "        return HuggingFaceCIFAR(hf_dataset, transform=transform)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CIFAR-100-LT: {e}\")\n",
    "        print(\"Falling back to creating custom imbalanced dataset...\")\n",
    "        return None\n",
    "\n",
    "print(\"Dataset loading functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Experiment Runner\n",
    "\n",
    "Run experiments with existing imbalanced datasets or custom ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "\n",
    "# Add paths\n",
    "sys.path.append('.')\n",
    "sys.path.append('imbalanced-exp')\n",
    "\n",
    "from CNN import CNN\n",
    "from VisionTransormer import VisionTransformer\n",
    "from create_imbalanced_dataset import ImbalancedCIFAR10, create_long_tail_imbalance, create_step_imbalance\n",
    "from evaluate import evaluate_model, print_metrics, plot_confusion_matrix, plot_per_class_metrics\n",
    "\n",
    "def train_model(model, train_loader, num_epochs, device, print_freq=10):\n",
    "    \"\"\"Train model and return training history\"\"\"\n",
    "    history = {'loss': [], 'epoch': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            loss = model.training_step(batch)\n",
    "            epoch_losses.append(loss.item())\n",
    "            \n",
    "            if batch_idx % print_freq == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        history['loss'].append(avg_loss)\n",
    "        history['epoch'].append(epoch)\n",
    "        print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "def plot_training_curves(history_cnn, history_vit, save_path):\n",
    "    \"\"\"Plot training loss curves\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history_cnn['epoch'], history_cnn['loss'], label='CNN', marker='o')\n",
    "    plt.plot(history_vit['epoch'], history_vit['loss'], label='ViT', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Training Loss Curves - Imbalanced Dataset')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Training curves saved to {save_path}\")\n",
    "\n",
    "print(\"Experiment functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Experiment with Existing Imbalanced Dataset\n",
    "\n",
    "### Option A: CIFAR-10-LT (Long-Tailed CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASET_TYPE = 'cifar10_lt'  # Options: 'cifar10_lt', 'cifar100_lt', 'custom'\n",
    "IMBALANCE_FACTOR = 100  # For LT datasets: 10, 50, 100, 200\n",
    "NUM_EPOCHS = 10\n",
    "NUM_CLASSES = 10  # 10 for CIFAR-10, 100 for CIFAR-100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load config\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Setup transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Load existing imbalanced dataset\n",
    "if DATASET_TYPE == 'cifar10_lt':\n",
    "    train_dataset = load_cifar10_lt(imbalance_factor=IMBALANCE_FACTOR, split='train', transform=transform)\n",
    "    test_dataset = load_cifar10_lt(imbalance_factor=IMBALANCE_FACTOR, split='test', transform=transform)\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "elif DATASET_TYPE == 'cifar100_lt':\n",
    "    train_dataset = load_cifar100_lt(imbalance_factor=IMBALANCE_FACTOR, split='train', transform=transform)\n",
    "    test_dataset = load_cifar100_lt(imbalance_factor=IMBALANCE_FACTOR, split='test', transform=transform)\n",
    "    NUM_CLASSES = 100\n",
    "    class_names = [f'class_{i}' for i in range(100)]\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset type: {DATASET_TYPE}\")\n",
    "\n",
    "if train_dataset is None or test_dataset is None:\n",
    "    print(\"Failed to load dataset. Please check the dataset name or use custom dataset option.\")\n",
    "else:\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Analyze class distribution\n",
    "    from collections import Counter\n",
    "    train_labels = [train_dataset[i][1] for i in range(len(train_dataset))]\n",
    "    class_distribution = Counter(train_labels)\n",
    "    print(\"\\nClass distribution in training set:\")\n",
    "    for cls in sorted(class_distribution.keys()):\n",
    "        print(f\"  Class {cls}: {class_distribution[cls]} samples\")\n",
    "    \n",
    "    print(f\"\\nTotal training samples: {len(train_dataset)}\")\n",
    "    print(f\"Total test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: CIFAR-100-LT (Long-Tailed CIFAR-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and modify to use CIFAR-100-LT\n",
    "# DATASET_TYPE = 'cifar100_lt'\n",
    "# IMBALANCE_FACTOR = 100\n",
    "# NUM_CLASSES = 100\n",
    "# \n",
    "# train_dataset = load_cifar100_lt(imbalance_factor=IMBALANCE_FACTOR, split='train', transform=transform)\n",
    "# test_dataset = load_cifar100_lt(imbalance_factor=IMBALANCE_FACTOR, split='test', transform=transform)\n",
    "# \n",
    "# # Update CNN and ViT to output 100 classes\n",
    "# # (You'll need to modify the final layer in CNN.py and VisionTransormer.py)\n",
    "\n",
    "print(\"To use CIFAR-100-LT, uncomment the code above and ensure models support 100 classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option C: Custom Imbalanced Dataset (Create from CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to create custom imbalanced dataset\n",
    "# from create_imbalanced_dataset import create_long_tail_imbalance, create_step_imbalance\n",
    "# \n",
    "# # Load full CIFAR-10\n",
    "# full_train = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "# full_test = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "# \n",
    "# # Create imbalanced version\n",
    "# train_indices, class_dist = create_long_tail_imbalance(full_train, imbalance_ratio=0.1)\n",
    "# train_dataset = ImbalancedCIFAR10(full_train, train_indices)\n",
    "# test_dataset = full_test\n",
    "# \n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"To use custom imbalanced dataset, uncomment the code above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"imbalanced-exp/results/{DATASET_TYPE}_if{IMBALANCE_FACTOR}_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Initializing CNN (ResNet)\")\n",
    "print(\"=\"*60)\n",
    "cnn_config = config['cnn'].copy()\n",
    "cnn_model = CNN(config=cnn_config, device=device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Initializing Vision Transformer\")\n",
    "print(\"=\"*60)\n",
    "vit_config = config['vision_transformer'].copy()\n",
    "vit_model = VisionTransformer(config=vit_config, device=device)\n",
    "\n",
    "# Train CNN\n",
    "print(\"\\nTraining CNN...\")\n",
    "cnn_history = train_model(cnn_model, train_loader, NUM_EPOCHS, device, \n",
    "                         print_freq=config.get('print_batch_frequency', 10))\n",
    "\n",
    "# Train ViT\n",
    "print(\"\\nTraining ViT...\")\n",
    "vit_history = train_model(vit_model, train_loader, NUM_EPOCHS, device,\n",
    "                         print_freq=config.get('print_batch_frequency', 10))\n",
    "\n",
    "# Evaluate models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluating CNN\")\n",
    "print(\"=\"*60)\n",
    "cnn_metrics = evaluate_model(cnn_model, test_loader, device, num_classes=NUM_CLASSES, class_names=class_names)\n",
    "print_metrics(cnn_metrics, \"CNN (ResNet)\", class_names)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluating Vision Transformer\")\n",
    "print(\"=\"*60)\n",
    "vit_metrics = evaluate_model(vit_model, test_loader, device, num_classes=NUM_CLASSES, class_names=class_names)\n",
    "print_metrics(vit_metrics, \"Vision Transformer\", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results and Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    'experiment_config': {\n",
    "        'dataset_type': DATASET_TYPE,\n",
    "        'imbalance_factor': IMBALANCE_FACTOR,\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'class_distribution': dict(class_distribution) if 'class_distribution' in locals() else {}\n",
    "    },\n",
    "    'cnn_metrics': {\n",
    "        'test_loss': float(cnn_metrics['test_loss']),\n",
    "        'accuracy': float(cnn_metrics['accuracy']),\n",
    "        'f1_macro': float(cnn_metrics['f1_macro']),\n",
    "        'f1_weighted': float(cnn_metrics['f1_weighted']),\n",
    "        'f1_per_class': [float(x) for x in cnn_metrics['f1_per_class']],\n",
    "        'per_class_recall': [float(x) for x in cnn_metrics['per_class_recall']],\n",
    "        'per_class_precision': [float(x) for x in cnn_metrics['per_class_precision']]\n",
    "    },\n",
    "    'vit_metrics': {\n",
    "        'test_loss': float(vit_metrics['test_loss']),\n",
    "        'accuracy': float(vit_metrics['accuracy']),\n",
    "        'f1_macro': float(vit_metrics['f1_macro']),\n",
    "        'f1_weighted': float(vit_metrics['f1_weighted']),\n",
    "        'f1_per_class': [float(x) for x in vit_metrics['f1_per_class']],\n",
    "        'per_class_recall': [float(x) for x in vit_metrics['per_class_recall']],\n",
    "        'per_class_precision': [float(x) for x in vit_metrics['per_class_precision']]\n",
    "    },\n",
    "    'training_history': {\n",
    "        'cnn_loss': [float(x) for x in cnn_history['loss']],\n",
    "        'vit_loss': [float(x) for x in vit_history['loss']]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{output_dir}/results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Generate visualizations\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "\n",
    "# Confusion matrices\n",
    "plot_confusion_matrix(\n",
    "    cnn_metrics['confusion_matrix'], \n",
    "    class_names,\n",
    "    f'{output_dir}/confusion_matrix_cnn.png',\n",
    "    'CNN (ResNet)'\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    vit_metrics['confusion_matrix'],\n",
    "    class_names,\n",
    "    f'{output_dir}/confusion_matrix_vit.png',\n",
    "    'Vision Transformer'\n",
    ")\n",
    "\n",
    "# Per-class metrics comparison\n",
    "plot_per_class_metrics(\n",
    "    cnn_metrics, vit_metrics, class_names,\n",
    "    f'{output_dir}/per_class_metrics_comparison.png'\n",
    ")\n",
    "\n",
    "# Training curves\n",
    "plot_training_curves(\n",
    "    cnn_history, vit_history,\n",
    "    f'{output_dir}/training_curves.png'\n",
    ")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<25} {'CNN':<15} {'ViT':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Accuracy':<25} {cnn_metrics['accuracy']*100:>6.2f}%      {vit_metrics['accuracy']*100:>6.2f}%\")\n",
    "print(f\"{'F1-Score (Macro)':<25} {cnn_metrics['f1_macro']:>6.4f}      {vit_metrics['f1_macro']:>6.4f}\")\n",
    "print(f\"{'F1-Score (Weighted)':<25} {cnn_metrics['f1_weighted']:>6.4f}      {vit_metrics['f1_weighted']:>6.4f}\")\n",
    "print(f\"{'Test Loss':<25} {cnn_metrics['test_loss']:>6.4f}      {vit_metrics['test_loss']:>6.4f}\")\n",
    "\n",
    "print(f\"\\nAll results saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import json\n",
    "\n",
    "# Display results\n",
    "with open(f'{output_dir}/results.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(\"Experiment Results:\")\n",
    "print(f\"Dataset: {results['experiment_config']['dataset_type']}\")\n",
    "print(f\"Imbalance Factor: {results['experiment_config']['imbalance_factor']}\")\n",
    "print(f\"\\nCNN Accuracy: {results['cnn_metrics']['accuracy']*100:.2f}%\")\n",
    "print(f\"ViT Accuracy: {results['vit_metrics']['accuracy']*100:.2f}%\")\n",
    "print(f\"CNN F1-Macro: {results['cnn_metrics']['f1_macro']:.4f}\")\n",
    "print(f\"ViT F1-Macro: {results['vit_metrics']['f1_macro']:.4f}\")\n",
    "\n",
    "# Display plots\n",
    "plot_files = [\n",
    "    'confusion_matrix_cnn.png',\n",
    "    'confusion_matrix_vit.png',\n",
    "    'per_class_metrics_comparison.png',\n",
    "    'training_curves.png'\n",
    "]\n",
    "\n",
    "for plot_file in plot_files:\n",
    "    plot_path = f\"{output_dir}/{plot_file}\"\n",
    "    if os.path.exists(plot_path):\n",
    "        print(f\"\\nDisplaying {plot_file}:\")\n",
    "        display(Image(plot_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Create zip file\n",
    "zip_filename = f\"{os.path.basename(output_dir)}.zip\"\n",
    "shutil.make_archive(output_dir, 'zip', output_dir)\n",
    "\n",
    "# Download\n",
    "files.download(f\"{output_dir}.zip\")\n",
    "print(f\"Downloaded {zip_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save to Google Drive (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy results to Drive\n",
    "drive_path = '/content/drive/MyDrive/imbalanced_experiments'\n",
    "!mkdir -p {drive_path}\n",
    "!cp -r {output_dir} {drive_path}/\n",
    "print(f\"Results saved to {drive_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
